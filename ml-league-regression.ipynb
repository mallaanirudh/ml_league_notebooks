{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":89543,"databundleVersionId":10322696,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:40.513715Z","iopub.execute_input":"2025-06-30T05:27:40.514029Z","iopub.status.idle":"2025-06-30T05:27:40.906564Z","shell.execute_reply.started":"2025-06-30T05:27:40.514000Z","shell.execute_reply":"2025-06-30T05:27:40.905515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ntrain = pd.read_csv('/kaggle/input/ml-league-supervised-learning-competition/train.csv')\ntest = pd.read_csv('/kaggle/input/ml-league-supervised-learning-competition/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/ml-league-supervised-learning-competition/sample_submission.csv')\n\n# Show a quick preview\ntrain.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:40.907305Z","iopub.execute_input":"2025-06-30T05:27:40.907735Z","iopub.status.idle":"2025-06-30T05:27:41.007799Z","shell.execute_reply.started":"2025-06-30T05:27:40.907706Z","shell.execute_reply":"2025-06-30T05:27:41.006862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FIRT SUBMISSION USING XGBOOST ","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:41.008953Z","iopub.execute_input":"2025-06-30T05:27:41.009282Z","iopub.status.idle":"2025-06-30T05:27:41.683734Z","shell.execute_reply.started":"2025-06-30T05:27:41.009254Z","shell.execute_reply":"2025-06-30T05:27:41.682850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Droppping unused columns\ndrop_cols = ['id', 'Row#']  \nX = train.drop(columns=drop_cols + ['yield'])\ny = train['yield']\n\nX_test = test.drop(columns=drop_cols)\n\n# Filling the  missing values \nX.fillna(X.mean(), inplace=True)\nX_test.fillna(X_test.mean(), inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:41.684419Z","iopub.execute_input":"2025-06-30T05:27:41.684849Z","iopub.status.idle":"2025-06-30T05:27:41.764768Z","shell.execute_reply.started":"2025-06-30T05:27:41.684824Z","shell.execute_reply":"2025-06-30T05:27:41.763833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:41.767684Z","iopub.execute_input":"2025-06-30T05:27:41.767977Z","iopub.status.idle":"2025-06-30T05:27:41.778198Z","shell.execute_reply.started":"2025-06-30T05:27:41.767954Z","shell.execute_reply":"2025-06-30T05:27:41.777235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = XGBRegressor(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# Validation predictions\nval_preds = model.predict(X_val)\nval_mae = mean_absolute_error(y_val, val_preds)\nprint(f\"Validation MAE: {val_mae:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:41.779295Z","iopub.execute_input":"2025-06-30T05:27:41.779913Z","iopub.status.idle":"2025-06-30T05:27:43.083150Z","shell.execute_reply.started":"2025-06-30T05:27:41.779883Z","shell.execute_reply":"2025-06-30T05:27:43.082183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_preds = model.predict(X_test)\n\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'yield': test_preds})\n\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:43.084069Z","iopub.execute_input":"2025-06-30T05:27:43.084408Z","iopub.status.idle":"2025-06-30T05:27:43.141024Z","shell.execute_reply.started":"2025-06-30T05:27:43.084388Z","shell.execute_reply":"2025-06-30T05:27:43.140021Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PANDAS PROFILIG DIDNT WORK THREE TIMES, KAGGLE BECAME UNREPONSIVE SO DID EDA MANUALLY.","metadata":{}},{"cell_type":"markdown","source":"# EDA PIPELINE","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:43.142005Z","iopub.execute_input":"2025-06-30T05:27:43.142273Z","iopub.status.idle":"2025-06-30T05:27:43.556520Z","shell.execute_reply.started":"2025-06-30T05:27:43.142250Z","shell.execute_reply":"2025-06-30T05:27:43.555678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Basic Info + Missing Values","metadata":{}},{"cell_type":"code","source":"print(\" \\nShape:\", train.shape)\nprint(\"\\n Data Types:\\n\", train.dtypes)\nprint(\"\\n Missing Values:\\n\", train.isnull().sum().sort_values(ascending=False))\nprint(\"\\n Descriptive Stats:\\n\", train.describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:43.557433Z","iopub.execute_input":"2025-06-30T05:27:43.557826Z","iopub.status.idle":"2025-06-30T05:27:43.614292Z","shell.execute_reply.started":"2025-06-30T05:27:43.557798Z","shell.execute_reply":"2025-06-30T05:27:43.613513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Distribution & Skewness of Features","metadata":{}},{"cell_type":"code","source":"numeric_features = train.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_features.remove(\"yield\")  # Excluding target for now\n\n# Skewness\nskewed = train[numeric_features].skew().sort_values(ascending=False)\nprint(\"\\n🧬 Feature Skewness:\\n\", skewed)\n\n# distributions\nfor col in numeric_features:\n    plt.figure(figsize=(6, 4))\n    sns.histplot(train[col], kde=True, bins=30)\n    plt.title(f\"Distribution of {col}\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:43.615094Z","iopub.execute_input":"2025-06-30T05:27:43.615335Z","iopub.status.idle":"2025-06-30T05:27:48.417465Z","shell.execute_reply.started":"2025-06-30T05:27:43.615316Z","shell.execute_reply":"2025-06-30T05:27:48.414937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# correlation map","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14, 10))\ncorr = train.corr()\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\nplt.title(\" Correlation Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.418164Z","iopub.status.idle":"2025-06-30T05:27:48.418483Z","shell.execute_reply.started":"2025-06-30T05:27:48.418345Z","shell.execute_reply":"2025-06-30T05:27:48.418359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_corr = corr[\"yield\"].sort_values(ascending=False)\nprint(\" Features Most Correlated with 'yield':\\n\", target_corr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.419359Z","iopub.status.idle":"2025-06-30T05:27:48.419747Z","shell.execute_reply.started":"2025-06-30T05:27:48.419569Z","shell.execute_reply":"2025-06-30T05:27:48.419586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# outliers","metadata":{}},{"cell_type":"code","source":"for col in numeric_features:\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(x=train[col])\n    plt.title(f\"Boxplot of {col}\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.421095Z","iopub.status.idle":"2025-06-30T05:27:48.421591Z","shell.execute_reply.started":"2025-06-30T05:27:48.421443Z","shell.execute_reply":"2025-06-30T05:27:48.421459Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Relationships to Target","metadata":{}},{"cell_type":"code","source":"for col in numeric_features:\n    plt.figure(figsize=(6, 4))\n    sns.scatterplot(x=train[col], y=train['yield'])\n    plt.title(f\"{col} vs Yield\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.422455Z","iopub.status.idle":"2025-06-30T05:27:48.422713Z","shell.execute_reply.started":"2025-06-30T05:27:48.422595Z","shell.execute_reply":"2025-06-30T05:27:48.422606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# statistical test\n\nThe Shapiro–Wilk test is a statistical method used to determine whether a given dataset follows a normal distribution (also known as a Gaussian distribution). This test is commonly applied in data analysis to check one of the key assumptions of many statistical models, such as linear regression — that the residuals or inputs are normally distributed. The test works by comparing the order statistics (sorted values) of the sample to what would be expected from a normal distribution. It returns a test statistic and a p-value: if the p-value is less than a chosen significance level (typically 0.05), the null hypothesis — that the data is normally distributed — is rejected. In practice, this means that a low p-value indicates non-normal data, while a high p-value suggests the data is consistent with a normal distribution. The Shapiro–Wilk test is most effective on small to moderately sized samples (usually n < 5000).","metadata":{},"attachments":{}},{"cell_type":"code","source":"# Shapiro-Wilk test for normality\nfor col in numeric_features:\n    stat, p = stats.shapiro(train[col].dropna())\n    print(f\"{col}: p={p:.4f} {'(Not normal)' if p < 0.05 else '(Probably normal)'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.424167Z","iopub.status.idle":"2025-06-30T05:27:48.424542Z","shell.execute_reply.started":"2025-06-30T05:27:48.424360Z","shell.execute_reply":"2025-06-30T05:27:48.424377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nX = train.drop(columns=['id', 'Row#', 'yield'])\ny = train['yield']\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X, y)\n\nimportances = pd.Series(model.feature_importances_, index=X.columns)\nimportances.sort_values().plot(kind='barh', figsize=(10, 8), title=\"Feature Importance (Random Forest)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.425956Z","iopub.status.idle":"2025-06-30T05:27:48.426339Z","shell.execute_reply.started":"2025-06-30T05:27:48.426142Z","shell.execute_reply":"2025-06-30T05:27:48.426160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\n# Define data\nfeatures = ['fruitset', 'seeds', 'fruitmass']\nX =train[features]\ny =train['yield']\n\n# Train/Val Split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train LightGBM\nmodel = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05)\nmodel.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n\n# Evaluate\ny_pred = model.predict(X_val)\nmae = mean_absolute_error(y_val, y_pred)\nprint(f\"📉 MAE: {mae:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.428049Z","iopub.status.idle":"2025-06-30T05:27:48.428465Z","shell.execute_reply.started":"2025-06-30T05:27:48.428274Z","shell.execute_reply":"2025-06-30T05:27:48.428294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/test.csv\")\nX_test = test[['fruitset', 'seeds', 'fruitmass']]\ny_test_pred = model.predict(X_test)\nsubmission = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/sample_submission.csv\")\nsubmission['yield'] = y_test_pred\nsubmission.to_csv(\"submission2.csv\", index=False)\nprint(\" submission.csv saved successfully!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.429932Z","iopub.status.idle":"2025-06-30T05:27:48.430307Z","shell.execute_reply.started":"2025-06-30T05:27:48.430090Z","shell.execute_reply":"2025-06-30T05:27:48.430102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.431484Z","iopub.status.idle":"2025-06-30T05:27:48.431912Z","shell.execute_reply.started":"2025-06-30T05:27:48.431689Z","shell.execute_reply":"2025-06-30T05:27:48.431706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBRegressor\n\n# Load the training data\ntrain = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/train.csv\")\n\n# Features and target\nfeatures = ['fruitset', 'seeds', 'fruitmass']\nX = train[features]\ny = train['yield']\n\n# Train/validation split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nresults = {}\n\n# XGBoost\nxgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05, random_state=42)\nxgb_model.fit(X_train, y_train)\nxgb_preds = xgb_model.predict(X_val)\nresults['XGBoost'] = mean_absolute_error(y_val, xgb_preds)\n\n# Random Forest\nrf_model = RandomForestRegressor(n_estimators=300, random_state=42)\nrf_model.fit(X_train, y_train)\nrf_preds = rf_model.predict(X_val)\nresults['RandomForest'] = mean_absolute_error(y_val, rf_preds)\n\n# HistGradientBoosting\nhgb_model = HistGradientBoostingRegressor(random_state=42)\nhgb_model.fit(X_train, y_train)\nhgb_preds = hgb_model.predict(X_val)\nresults['HistGradientBoosting'] = mean_absolute_error(y_val, hgb_preds)\n\n# KNN (with scaling)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nknn_model = KNeighborsRegressor(n_neighbors=5)\nknn_model.fit(X_train_scaled, y_train)\nknn_preds = knn_model.predict(X_val_scaled)\nresults['KNN'] = mean_absolute_error(y_val, knn_preds)\n\n# Display all MAEs\nfor model, mae in results.items():\n    print(f\"{model} MAE: {mae:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.433153Z","iopub.status.idle":"2025-06-30T05:27:48.433410Z","shell.execute_reply.started":"2025-06-30T05:27:48.433294Z","shell.execute_reply":"2025-06-30T05:27:48.433305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/test.csv\")\nX_test = test[['fruitset', 'seeds', 'fruitmass']]\ny_test_pred = hgb_model.predict(X_test)\nsubmission = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/sample_submission.csv\")\nsubmission['yield'] = y_test_pred\nsubmission.to_csv(\"submission3.csv\", index=False)\nprint(\" submission3.csv saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.434175Z","iopub.status.idle":"2025-06-30T05:27:48.434426Z","shell.execute_reply.started":"2025-06-30T05:27:48.434309Z","shell.execute_reply":"2025-06-30T05:27:48.434320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom ydata_profiling import ProfileReport\n\n# Load your dataset\ndf = pd.read_csv('/kaggle/input/ml-league-supervised-learning-competition/train.csv')\n\n# Generate the report\nprofile = ProfileReport(df, title=\"EDA Report\", explorative=True,minimal = True)\n\n# Save or display it\nprofile.to_file(\"eda_report.html\")\n# OR in notebook\nprofile.to_notebook_iframe()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.435600Z","iopub.status.idle":"2025-06-30T05:27:48.435944Z","shell.execute_reply.started":"2025-06-30T05:27:48.435771Z","shell.execute_reply":"2025-06-30T05:27:48.435808Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# submisson 4","metadata":{}},{"cell_type":"code","source":"\ntrain = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/test.csv\")\n\n\nfeatures = ['fruitset', 'seeds', 'fruitmass']\n\n\nfor df in [train, test]:\n    df['fruitset_x_mass'] = df['fruitset'] * df['fruitmass']\n    df['seeds_div_mass'] = df['seeds'] / (df['fruitmass'] + 1e-6)  # avoid divide by zero\n    df['fruit_density'] = df['fruitmass'] / (df['seeds'] + 1e-6)\n\nnew_features = features + ['fruitset_x_mass', 'seeds_div_mass', 'fruit_density']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.436881Z","iopub.status.idle":"2025-06-30T05:27:48.437139Z","shell.execute_reply.started":"2025-06-30T05:27:48.437027Z","shell.execute_reply":"2025-06-30T05:27:48.437037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nbest_model.fit(X, y)\n\n\n\nX_test = test[new_features]\ntest_preds = best_model.predict(X_test)\n\n\nsample_submission = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/sample_submission.csv\")\nsample_submission['yield'] = test_preds\nsample_submission.to_csv(\"submission4.csv\", index=False)\n\nprint(\"submission4.csv saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.437859Z","iopub.status.idle":"2025-06-30T05:27:48.438167Z","shell.execute_reply.started":"2025-06-30T05:27:48.437996Z","shell.execute_reply":"2025-06-30T05:27:48.438011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install optuna\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.439551Z","iopub.status.idle":"2025-06-30T05:27:48.439940Z","shell.execute_reply.started":"2025-06-30T05:27:48.439729Z","shell.execute_reply":"2025-06-30T05:27:48.439744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import make_scorer, mean_absolute_error\nimport numpy as np\n\nX = train[new_features]\ny = train['yield']\n\ndef objective(trial):\n    model = HistGradientBoostingRegressor(\n        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        max_iter=trial.suggest_int(\"max_iter\", 200, 1000),\n        max_leaf_nodes=trial.suggest_int(\"max_leaf_nodes\", 15, 63),\n        min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 5, 50),\n        l2_regularization=trial.suggest_float(\"l2_regularization\", 0.0, 1.0),\n        random_state=42\n    )\n    score = cross_val_score(model, X, y, cv=3,\n                            scoring=make_scorer(mean_absolute_error)).mean()\n    return score\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=30)\n\nprint(\"Best params:\", study.best_params)\nprint(\"Best MAE:\", study.best_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.441178Z","iopub.status.idle":"2025-06-30T05:27:48.441420Z","shell.execute_reply.started":"2025-06-30T05:27:48.441304Z","shell.execute_reply":"2025-06-30T05:27:48.441313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# stacking model","metadata":{}},{"cell_type":"code","source":"\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, StackingRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom xgboost import XGBRegressor\n\n\ntrain = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/sample_submission.csv\")\n\n\nfeatures = ['fruitset', 'seeds', 'fruitmass']\nfor df in [train, test]:\n    df['fruitset_x_mass'] = df['fruitset'] * df['fruitmass']\n    df['seeds_div_mass'] = df['seeds'] / (df['fruitmass'] + 1e-6)\n    df['fruit_density'] = df['fruitmass'] / (df['seeds'] + 1e-6)\n\nnew_features = features + ['fruitset_x_mass', 'seeds_div_mass', 'fruit_density']\nX = train[new_features]\ny = train['yield']\nX_test = test[new_features]\n\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nbase_models = [\n    ('hgb', HistGradientBoostingRegressor(random_state=42)),\n    ('xgb', XGBRegressor(n_estimators=300, learning_rate=0.05, random_state=42)),\n    ('rf', RandomForestRegressor(n_estimators=300, random_state=42))\n]\n\nstacked_model = StackingRegressor(\n    estimators=base_models,\n    final_estimator=RidgeCV(),\n    cv=3,\n    n_jobs=-1\n)\n\n\nstacked_model.fit(X_train, y_train)\n\n\nval_preds = stacked_model.predict(X_val)\nval_mae = mean_absolute_error(y_val, val_preds)\nprint(f\"📉 Stacked MAE: {val_mae:.4f}\")\n\n\ntest_preds = stacked_model.predict(X_test)\nsubmission['yield'] = test_preds\nsubmission.to_csv(\"submission_stacked.csv\", index=False)\nprint(\" submission_stacked.csv saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.442445Z","iopub.status.idle":"2025-06-30T05:27:48.442718Z","shell.execute_reply.started":"2025-06-30T05:27:48.442564Z","shell.execute_reply":"2025-06-30T05:27:48.442573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n\n\ntrain = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/sample_submission.csv\")\n\n\nfor df in [train, test]:\n    df['fruitset_x_mass'] = df['fruitset'] * df['fruitmass']\n    df['seeds_div_mass'] = df['seeds'] / (df['fruitmass'] + 1e-6)\n    df['fruit_density'] = df['fruitmass'] / (df['seeds'] + 1e-6)\n\nfeatures = ['fruitset', 'seeds', 'fruitmass', 'fruitset_x_mass', 'seeds_div_mass', 'fruit_density']\nX = train[features]\ny = train['yield']\nX_test = test[features]\n\n\nbest_model = HistGradientBoostingRegressor(\n    learning_rate=0.01096,\n    max_iter=607,\n    max_leaf_nodes=27,\n    min_samples_leaf=6,\n    l2_regularization=0.609,\n    random_state=42\n)\nbest_model.fit(X, y)\n\n\n#joblib.dump(best_model, \"optuna_best_hgb_model.pkl\")\n\n\npreds = best_model.predict(X_test)\nsubmission['yield'] = preds\nsubmission.to_csv(\"submission_optuna_final.csv\", index=False)\n\nprint(\" Model and submission saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.443427Z","iopub.status.idle":"2025-06-30T05:27:48.443689Z","shell.execute_reply.started":"2025-06-30T05:27:48.443553Z","shell.execute_reply":"2025-06-30T05:27:48.443563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\ntrain = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/sample_submission.csv\")\n\nfor df in [train, test]:\n    df['fruitset_x_mass'] = df['fruitset'] * df['fruitmass']\n    df['fruitset_x_seeds'] = df['fruitset'] * df['seeds']\n    df['mass_x_seeds'] = df['fruitmass'] * df['seeds']\n    df['combo_feature'] = df['fruitset'] * df['fruitmass'] * df['seeds']\n\nfeatures = ['fruitset', 'fruitmass', 'seeds', 'fruitset_x_mass', 'fruitset_x_seeds', 'mass_x_seeds', 'combo_feature']\nX = train[features]\ny = train['yield']\nX_test = test[features]\n\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = HistGradientBoostingRegressor(\n    learning_rate=0.01096,\n    max_iter=607,\n    max_leaf_nodes=27,\n    min_samples_leaf=6,\n    l2_regularization=0.609,\n    random_state=42\n)\nmodel.fit(X_train, y_train)\nval_mae = mean_absolute_error(y_val, model.predict(X_val))\nprint(f\"📉 Validation MAE: {val_mae:.4f}\")\n\n\nmodel.fit(X, y)\npreds = model.predict(X_test)\n\n#joblib.dump(model, \"final_engineered_model.pkl\")\n\n\nsubmission['yield'] = preds\nsubmission.to_csv(\"submission_final_engineered.csv\", index=False)\nprint(\"submission_6.csv and model saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.444968Z","iopub.status.idle":"2025-06-30T05:27:48.445199Z","shell.execute_reply.started":"2025-06-30T05:27:48.445091Z","shell.execute_reply":"2025-06-30T05:27:48.445100Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n\n\n\n\ndrop_cols = ['clonesize', 'Row#', 'RainingDays', 'AverageRainingDays', 'honeybee', 'andrena', 'bumbles']\ntrain = train.drop(columns=drop_cols, errors='ignore')\ntest = test.drop(columns=drop_cols, errors='ignore')\n\n\nfor df in [train, test]:\n    df['fruitset_x_seeds'] = df['fruitset'] * df['seeds']\n    df['fruitmass_x_seeds'] = df['fruitmass'] * df['seeds']\n    df['combo_feature'] = df['fruitset'] * df['fruitmass'] * df['seeds']\n    df['fruit_density'] = df['fruitmass'] / (df['seeds'] + 1e-6)\n    df['osmia_boost'] = df['fruitset'] * df['osmia']\n\n\nfeatures = [\n    'fruitset', 'fruitmass', 'seeds', 'osmia',\n    'fruitset_x_seeds', 'fruitmass_x_seeds',\n    'combo_feature', 'fruit_density', 'osmia_boost'\n]\n\n\nX = train[features]\ny = train['yield']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nmodel = HistGradientBoostingRegressor(\n    learning_rate=0.01,\n    max_iter=650,\n    max_leaf_nodes=27,\n    min_samples_leaf=6,\n    l2_regularization=0.6,\n    random_state=42\n)\n\n\nmodel.fit(X_train, y_train)\nval_preds = model.predict(X_val)\nval_mae = mean_absolute_error(y_val, val_preds)\nprint(f\" Validation MAE: {val_mae:.4f}\")\n\n\nmodel.fit(X, y)\ntest_preds = model.predict(test[features])\n\n# Save outputs\nsubmission['yield'] = test_preds\nsubmission.to_csv(\"submission_target_mae230.csv\", index=False)\njoblib.dump(model, \"model_target_mae230.pkl\")\nprint(\" submission_target_mae230.csv and model saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.445886Z","iopub.status.idle":"2025-06-30T05:27:48.446149Z","shell.execute_reply.started":"2025-06-30T05:27:48.446037Z","shell.execute_reply":"2025-06-30T05:27:48.446047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n\ndrop_cols = ['clonesize', 'Row#', 'RainingDays', 'AverageRainingDays', 'honeybee', 'andrena', 'bumbles']\ntrain.drop(columns=drop_cols, errors='ignore', inplace=True)\ntest.drop(columns=drop_cols, errors='ignore', inplace=True)\n\n\nfor col in ['fruitset', 'fruitmass', 'seeds', 'yield']:\n    q_low = train[col].quantile(0.01)\n    q_high = train[col].quantile(0.99)\n    train[col] = train[col].clip(lower=q_low, upper=q_high)\n\n\nfor df in [train, test]:\n    df['fruitset_x_seeds'] = df['fruitset'] * df['seeds']\n    df['fruitmass_x_seeds'] = df['fruitmass'] * df['seeds']\n    df['combo_feature'] = df['fruitset'] * df['fruitmass'] * df['seeds']\n    df['fruit_density'] = df['fruitmass'] / (df['seeds'] + 1e-6)\n    df['osmia_boost'] = df['fruitset'] * df['osmia']\n    df['mass_to_set'] = df['fruitmass'] / (df['fruitset'] + 1e-6)\n    df['log_seeds'] = np.log1p(df['seeds'])\n\n\nfeatures = [\n    'fruitset', 'fruitmass', 'seeds', 'osmia',\n    'fruitset_x_seeds', 'fruitmass_x_seeds',\n    'combo_feature', 'fruit_density', 'osmia_boost',\n    'mass_to_set', 'log_seeds'\n]\n\nX = train[features]\ny = train['yield']\nX_test = test[features]\n\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nmodel = HistGradientBoostingRegressor(\n    learning_rate=0.008,\n    max_iter=900,\n    max_leaf_nodes=31,\n    min_samples_leaf=5,\n    l2_regularization=0.4,\n    random_state=42\n)\n\n\nmodel.fit(X_train, y_train)\nval_preds = model.predict(X_val)\nval_mae = mean_absolute_error(y_val, val_preds)\nprint(f\" FINAL MAE: {val_mae:.4f}\")\n\nmodel.fit(X, y)\nsubmission['yield'] = model.predict(X_test)\nsubmission.to_csv(\"submission_ultra_tuned.csv\", index=False)\n\nprint(\"submission and model saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.447256Z","iopub.status.idle":"2025-06-30T05:27:48.447485Z","shell.execute_reply.started":"2025-06-30T05:27:48.447375Z","shell.execute_reply":"2025-06-30T05:27:48.447384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\nfeatures = ['fruitset', 'fruitmass', 'seeds', 'osmia']\nX = train[features]\ny = train['yield']\nX_test = test[features]\n\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nmodel = HistGradientBoostingRegressor(\n    learning_rate=0.03,\n    max_iter=600,\n    max_leaf_nodes=31,\n    random_state=42\n)\n\nmodel.fit(X_train, y_train)\nval_preds = model.predict(X_val)\nval_mae = mean_absolute_error(y_val, val_preds)\nprint(f\"🧪 MAE (Simple Model): {val_mae:.2f}\")\n\n# Retrain and predict\nmodel.fit(X, y)\nsubmission['yield'] = model.predict(X_test)\n#submission.to_csv(\"submission_simplified.csv\", index=False)\n\nprint(\"✅ Submission and model saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.448839Z","iopub.status.idle":"2025-06-30T05:27:48.449154Z","shell.execute_reply.started":"2025-06-30T05:27:48.448986Z","shell.execute_reply":"2025-06-30T05:27:48.449000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom sklearn.model_selection import KFold, cross_val_predict\n\n\ntrain['log_yield'] = np.log1p(train['yield'])\n\n\ndef engineer_features(df):\n    df['fruitset_x_seeds'] = df['fruitset'] * df['seeds']\n    df['fruitmass_x_seeds'] = df['fruitmass'] * df['seeds']\n    df['combo_feature'] = df['fruitset'] * df['fruitmass'] * df['seeds']\n    df['fruit_density'] = df['fruitmass'] / (df['seeds'] + 1e-6)\n    df['osmia_boost'] = df['fruitset'] * df['osmia']\n    df['bee_sum'] = df['honeybee'] + df['bumbles'] + df['andrena'] + df['osmia']\n    df['raining_index'] = df['RainingDays'] + df['AverageRainingDays']\n    return df\n\ntrain = engineer_features(train)\ntest = engineer_features(test)\n\n# Final feature list\nfeatures = [\n    'fruitset', 'fruitmass', 'seeds', 'osmia', 'honeybee', 'bumbles', 'andrena',\n    'RainingDays', 'AverageRainingDays',\n    'fruitset_x_seeds', 'fruitmass_x_seeds', 'combo_feature',\n    'fruit_density', 'osmia_boost', 'bee_sum', 'raining_index'\n]\n\nX = train[features]\ny_log = train['log_yield']\nX_test = test[features]\n\n# 5-Fold Cross Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nmodel = HistGradientBoostingRegressor(\n    learning_rate=0.03,\n    max_iter=600,\n    max_leaf_nodes=31,\n    min_samples_leaf=5,\n    random_state=42\n)\n\n# Out-of-fold predictions\noof_preds_log = cross_val_predict(model, X, y_log, cv=kf)\noof_preds = np.expm1(oof_preds_log)\ntrue_y = train['yield']\ncv_mae = mean_absolute_error(true_y, oof_preds)\nprint(f\" Cross-validated MAE: {cv_mae:.2f}\")\n\n# Final training and submission\nmodel.fit(X, y_log)\ntest_preds = np.expm1(model.predict(X_test))\nsubmission['yield'] = test_preds\nsubmission.to_csv(\"submission_generalized_cv.csv\", index=False)\n#joblib.dump(model, \"model_generalized_cv.pkl\")\nprint(\" Model and submission saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.450382Z","iopub.status.idle":"2025-06-30T05:27:48.450696Z","shell.execute_reply.started":"2025-06-30T05:27:48.450558Z","shell.execute_reply":"2025-06-30T05:27:48.450573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/sample_submission.csv\")\n\n\ntrain['log_yield'] = np.log1p(train['yield'])\n\nfeatures = ['fruitset','seeds']\n\nX = train[features]\ny_log = train['log_yield']\nX_test = test[features]\n\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nmodel = HistGradientBoostingRegressor(\n    learning_rate=0.03,\n    max_iter=600,\n    max_leaf_nodes=31,\n    min_samples_leaf=5,\n    random_state=42\n)\n\n\noof_preds_log = cross_val_predict(model, X, y_log, cv=kf)\noof_preds = np.expm1(oof_preds_log)\ncv_mae = mean_absolute_error(train['yield'], oof_preds)\nprint(f\"📉 Cross-Validated MAE: {cv_mae:.2f}\")\n\n\nmodel.fit(X, y_log) \nperm_importance = permutation_importance(model, X, y_log, n_repeats=5, random_state=42)\nimportances = perm_importance.importances_mean\n\nprint(\"\\n🔍 Permutation Feature Importance:\")\nfor feat, imp in sorted(zip(features, importances), key=lambda x: x[1], reverse=True):\n    print(f\"  {feat}: {imp:.4f}\")\n\n\nprint(f\"\\n FINAL CV MAE (Log-Transformed Model): {cv_mae:.2f}\")\ntest_preds_log = model.predict(X_test)\ntest_preds = np.expm1(test_preds_log)\nsubmission['yield'] = test_preds\nsubmission.to_csv(\"submission8.csv\", index=False)\n\nprint(\"\\nModel and submission saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.451833Z","iopub.status.idle":"2025-06-30T05:27:48.452181Z","shell.execute_reply.started":"2025-06-30T05:27:48.452006Z","shell.execute_reply":"2025-06-30T05:27:48.452021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, cross_val_predict\nfrom sklearn.metrics import mean_absolute_error\n\n\ntrain = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/ml-league-supervised-learning-competition/sample_submission.csv\")\n\n\ny = train['yield']\nfeatures = ['fruitset','seeds']\nX = train[features]\nX_test = test[features]\n\n\nmodel = XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=4,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    n_jobs=-1\n)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = cross_val_predict(model, X, y, cv=kf)\ncv_mae = mean_absolute_error(y, oof_preds)\nprint(f\" CV MAE (XGBoost Generalized): {cv_mae:.2f}\")\n\nmodel.fit(X, y)\nsubmission['yield'] = model.predict(X_test)\nsubmission.to_csv(\"submission_boosted_generalized.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:27:48.453219Z","iopub.status.idle":"2025-06-30T05:27:48.453571Z","shell.execute_reply.started":"2025-06-30T05:27:48.453385Z","shell.execute_reply":"2025-06-30T05:27:48.453401Z"}},"outputs":[],"execution_count":null}]}